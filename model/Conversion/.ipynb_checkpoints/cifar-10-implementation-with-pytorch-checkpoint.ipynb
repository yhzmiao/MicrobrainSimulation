{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![pytorch flow](https://cdn-images-1.medium.com/max/800/1*uZrS4KjAuSJQIJPgOiaJUg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "import os\n",
    "#print(os.listdir(\"../input\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_model1 = True\n",
    "run_model2 = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "def load_cifar10_data(filename):\n",
    "    with open('../Datasets/cifar-10-batches-py/'+ filename, 'rb') as file:\n",
    "        batch = pickle.load(file, encoding='latin1')\n",
    "\n",
    "    features = batch['data']\n",
    "    labels = batch['labels']\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load files\n",
    "batch_1, labels_1 = load_cifar10_data('data_batch_1')\n",
    "batch_2, labels_2 = load_cifar10_data('data_batch_2')\n",
    "batch_3, labels_3 = load_cifar10_data('data_batch_3')\n",
    "batch_4, labels_4 = load_cifar10_data('data_batch_4')\n",
    "batch_5, labels_5 = load_cifar10_data('data_batch_5')\n",
    "\n",
    "test, label_test = load_cifar10_data('test_batch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge files\n",
    "X_train = np.concatenate([batch_1,batch_2,batch_3,batch_4,batch_5], 0)\n",
    "Y_train = np.concatenate([labels_1,labels_2,labels_3,labels_4,labels_5], 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ('airplane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "def return_photo(batch_file):\n",
    "    assert batch_file.shape[1] == 3072\n",
    "    dim = np.sqrt(1024).astype(int)\n",
    "    r = batch_file[:, 0:1024].reshape(batch_file.shape[0], dim, dim, 1)\n",
    "    g = batch_file[:, 1024:2048].reshape(batch_file.shape[0], dim, dim, 1)\n",
    "    b = batch_file[:, 2048:3072].reshape(batch_file.shape[0], dim, dim, 1)\n",
    "    photo = np.concatenate([r,g,b], -1)\n",
    "    return photo\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = return_photo(X_train)\n",
    "X_test = return_photo(test)\n",
    "Y_test = np.array(label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 216x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_image(number, file, label, pred=None):\n",
    "    fig = plt.figure(figsize = (3,2))\n",
    "    #img = return_photo(batch_file)\n",
    "    plt.imshow(file[number])\n",
    "    if pred is None:\n",
    "        plt.title(classes[label[number]])\n",
    "    else:\n",
    "        plt.title('Label_true: ' + classes[label[number]] + '\\nLabel_pred: ' + classes[pred[number]])\n",
    "    \n",
    "plot_image(12345, X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    5000\n",
      "1    5000\n",
      "2    5000\n",
      "3    5000\n",
      "4    5000\n",
      "5    5000\n",
      "6    5000\n",
      "7    5000\n",
      "8    5000\n",
      "9    5000\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yhz/anaconda3/envs/SpikingJellyenv/lib/python3.7/site-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "  FutureWarning\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# The cifar-10 is designed to balance distribution that the counts for each classification are 5000\n",
    "import seaborn as sns\n",
    "sns.countplot(Y_train)\n",
    "hist_Y_train = pd.Series(Y_train).groupby(Y_train).count()\n",
    "print(hist_Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 若使用 CrossEntropyLoss為Loss function, 則不需在輸入前將標籤手動轉換成one-hot格式\n",
    "#\n",
    "#from sklearn import preprocessing\n",
    "#oh_encoder = preprocessing.OneHotEncoder(categories='auto')\n",
    "#oh_encoder.fit(Y_train.reshape(-1,1))\n",
    "\n",
    "### In pytorch we dont need to normalize here, the 'transform' will do it for us.\n",
    "#\n",
    "#X_train_nor = X_train.astype('float32') / 255.0\n",
    "#X_test_nor = X_test.astype('float32') / 255.0\n",
    "\n",
    "\n",
    "#Y_train_oh = oh_encoder.transform(Y_train.reshape(-1,1)).toarray()\n",
    "#Y_test_oh = oh_encoder.transform(Y_test.reshape(-1,1)).toarray()\n",
    "\n",
    "# print('One-hot:')\n",
    "# print(Y_train_oh[:5])\n",
    "# print('\\nLabel:')\n",
    "# print(Y_train[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (50000, 32, 32, 3)\n",
      "Y_train shape: (50000,)\n",
      "X_test shape: (10000, 32, 32, 3)\n",
      "Y_test shape: (10000,)\n"
     ]
    }
   ],
   "source": [
    "# Final check for dimensions before pre-pocessing\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('Y_train shape:', Y_train.shape)\n",
    "print('X_test shape:', X_test.shape)\n",
    "print('Y_test shape:', Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the validation set out\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train_split, X_val_split, Y_train_split, Y_val_split = train_test_split(\n",
    "    X_train, Y_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 自定義Dataset: \n",
    "### Prepare for training & testing dataset. Define dataset class.\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import random\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "\n",
    "# define the random seed for reproducible result\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "\n",
    "# 繼承自Dataset物件\n",
    "class CIFAR10_from_array(Dataset): \n",
    "    def __init__(self, data, label, transform=None):\n",
    "        ##############################################\n",
    "        ### Initialize paths, transforms, and so on\n",
    "        ##############################################\n",
    "        #self.data = torch.from_numpy(data).float()\n",
    "        #self.label = torch.from_numpy(label).long()\n",
    "        self.data = data\n",
    "        self.label = label\n",
    "        self.transform = transform\n",
    "        self.img_shape = data.shape\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        ##############################################\n",
    "        # 1. Read from file (using numpy.fromfile, PIL.Image.open)\n",
    "        # 2. Preprocess the data (torchvision.Transform).\n",
    "        # 3. Return the data (e.g. image and label)\n",
    "        ##############################################\n",
    "        \n",
    "        img = Image.fromarray(self.data[index])\n",
    "        label = self.label[index]\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        else:\n",
    "            img_to_tensor = transforms.ToTensor()\n",
    "            img = img_to_tensor(img)\n",
    "            #label = torch.from_numpy(label).long()\n",
    "        return img, label\n",
    "        \n",
    "    def __len__(self):\n",
    "        ##############################################\n",
    "        ### Indicate the total size of the dataset\n",
    "        ##############################################\n",
    "        return len(self.data)\n",
    "    \n",
    "    def plot_image(self, number):\n",
    "        file = self.data\n",
    "        label = self.label\n",
    "        fig = plt.figure(figsize = (3,2))\n",
    "        #img = return_photo(batch_file)\n",
    "        plt.imshow(file[number])\n",
    "        plt.title(classes[label[number]])\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CIFAR10_from_url(Dataset): \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize for R, G, B with img = img - mean / std\n",
    "def normalize_dataset(data):\n",
    "    mean = data.mean(axis=(0,1,2)) / 255.0\n",
    "    std = data.std(axis=(0,1,2)) / 255.0\n",
    "    normalize = transforms.Normalize(mean=mean, std=std)\n",
    "    return normalize\n",
    "\n",
    "\n",
    "# apply transforms to return img as tensor type\n",
    "# ToTensor: 函数接受PIL Image或numpy.ndarray，将其先由HWC转置为CHW格式，再转为float后每个像素除以255.\n",
    "# Notice that the order in the \"compose\" does matter\n",
    "train_transform_aug = transforms.Compose([\n",
    "    transforms.Resize((40, 40)),       #先調整至略大的影像，\n",
    "    transforms.RandomCrop((32, 32)),   #再隨機擷取至模型輸入之大小，是常用的影像增量技巧之一。\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ToTensor(),\n",
    "    normalize_dataset(X_train)\n",
    "])\n",
    "\n",
    "# Also use X_train in normalize since train/val sets should have same distribution\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    normalize_dataset(X_train)\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    normalize_dataset(X_test)\n",
    "])\n",
    "\n",
    "trainset = CIFAR10_from_array(data=X_train_split, label=Y_train_split, transform=train_transform_aug)\n",
    "valset = CIFAR10_from_array(data=X_val_split, label=Y_val_split, transform=val_transform)\n",
    "testset = CIFAR10_from_array(data=X_test, label=Y_test, transform=test_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data shape check\n",
      "training set:       (40000, 32, 32, 3)\n",
      "validation set:     (10000, 32, 32, 3)\n",
      "testing set:        (10000, 32, 32, 3)\n",
      "label numbers:      10\n"
     ]
    }
   ],
   "source": [
    "print('data shape check')\n",
    "print('training set:'.ljust(20) + '{}'.format(trainset.img_shape))\n",
    "print('validation set:'.ljust(20) + '{}'.format(valset.img_shape))\n",
    "print('testing set:'.ljust(20) + '{}'.format(testset.img_shape))\n",
    "print('label numbers:'.ljust(20) + '{}'.format(len(set(trainset.label))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put into the data loader\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 64\n",
    "num_workers = 1\n",
    "\n",
    "train_loader = DataLoader(dataset=trainset,\n",
    "                          batch_size=batch_size, \n",
    "                          shuffle=True,\n",
    "                          num_workers=num_workers)\n",
    "\n",
    "\n",
    "val_loader = DataLoader(dataset=valset,\n",
    "                          batch_size=batch_size, \n",
    "                          shuffle=False,\n",
    "                          num_workers=num_workers)\n",
    "\n",
    "test_loader = DataLoader(dataset=testset,\n",
    "                          batch_size=batch_size, \n",
    "                          shuffle=False,\n",
    "                          num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of image: torch.Size([64, 3, 32, 32])\n",
      "Type of image: torch.float32\n",
      "Size of label: torch.Size([64])\n",
      "Type of label: torch.int64\n"
     ]
    }
   ],
   "source": [
    "imgs, lbls = iter(train_loader).next()\n",
    "print ('Size of image:', imgs.size())  # batch_size*3*224*224\n",
    "print ('Type of image:', imgs.dtype)   # float32\n",
    "print ('Size of label:', lbls.size())  # batch_size\n",
    "print ('Type of label:', lbls.dtype)   # int64(long)\n",
    "\n",
    "### Or you can do this:\n",
    "#for imgs, lbls in train_loader:\n",
    "#    print ('Size of image:', imgs.size())  # batch_size*3*224*224\n",
    "#    print ('Type of image:', imgs.dtype)   # float32\n",
    "#    print ('Size of label:', lbls.size())  # batch_size\n",
    "#    print ('Type of label:', lbls.dtype)   # int64(long)\n",
    "#    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Model\n",
    "\n",
    "### Model with out augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torch.utils.data as Data\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=(2, 2), stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Build the CNN\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=6, kernel_size=5)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=(2,2), stride=2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "            \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        \n",
    "        ### view 等同 reshape. 差別在於pytorch中reshape是複製, 而view則是永遠指向源頭\n",
    "        # \"Unlike reshape, reshape always copies memory. view never copies memory.\"\n",
    "        #ref: z = torch.zeros(3, 2)\n",
    "        #>>>  x = z.view(2, 3)\n",
    "        #>>>  z.fill_(1)\n",
    "        #>>>  x\n",
    "        #    tensor([[1., 1., 1.],\n",
    "        #            [1., 1., 1.]]) \n",
    "        \n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "#display net architecture\n",
    "print(Net())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import time\n",
    "\n",
    "def update_info(idx, length, epoch_loss, acc, mode):\n",
    "    \n",
    "    if length >= 250:\n",
    "        update_size = int(length/250)\n",
    "    else:\n",
    "        update_size = 5\n",
    "    \n",
    "    if idx % update_size == 0 and idx != 0:\n",
    "        #print ('=', end=\"\")        \n",
    "        finish_rate = idx/length * 100\n",
    "        print (\"\\r   {} progress: {:.2f}%  ......  loss: {:.4f} , acc: {:.4f}\".\n",
    "               format(mode, finish_rate, epoch_loss/idx, acc), end=\"\", flush=True)\n",
    "        \n",
    "\n",
    "def val_per_epoch(model, loss_fn, dataloader, verbose):\n",
    "    # In validation, we only compute loss value\n",
    "    model.eval()\n",
    "    epoch_loss = 0.0\n",
    "    acc = 0.0\n",
    "    val_size = 0\n",
    "    with torch.no_grad(): \n",
    "        for i, (feature, target) in enumerate(dataloader):\n",
    "            \n",
    "            #feature, target = feature.to(device), target.to(device)\n",
    "            if torch.cuda.is_available():\n",
    "                feature = feature.cuda()\n",
    "                target = target.cuda()\n",
    "            \n",
    "            output = model(feature) #outputs.data.shape= batches_num * num_class\n",
    "            \n",
    "            #compute acc\n",
    "            _, pred = torch.max(output.data, dim=1) \n",
    "            correct = (pred == target).sum().item() #convert to number\n",
    "            val_size += target.size(0)\n",
    "            acc += correct\n",
    "            \n",
    "            \n",
    "            loss = loss_fn(output, target)\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "            \n",
    "            idx = i\n",
    "            length = len(dataloader)\n",
    "            \n",
    "            #display progress\n",
    "            if verbose:\n",
    "                update_info(idx, length, epoch_loss, acc/val_size, 'validating')\n",
    "                \n",
    "        acc = acc/val_size\n",
    "    print('')\n",
    "    return epoch_loss/len(dataloader), acc\n",
    "\n",
    "\n",
    "def train_per_epoch(model, loss_fn, dataloader, optimizer, verbose): \n",
    "    #train mode\n",
    "    model.train()\n",
    "    \n",
    "    #initialize loss\n",
    "    epoch_loss = 0.0\n",
    "    acc = 0.0\n",
    "    train_size = 0\n",
    "    \n",
    "    for i, (feature, target) in enumerate(dataloader):\n",
    "        #feature, target = feature.to(device), target.to(device)\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            feature = feature.cuda()\n",
    "            target = target.cuda()\n",
    "        \n",
    "        #set zero to the parameter gradients for initialization\n",
    "        optimizer.zero_grad()\n",
    "        output = model(feature)\n",
    "        loss = loss_fn(output, target)\n",
    "        \n",
    "        \n",
    "        #compute acc\n",
    "        _, pred = torch.max(output.data, dim=1) \n",
    "        correct = (pred == target).sum().item() #convert to number\n",
    "        train_size += target.size(0)\n",
    "        acc += correct\n",
    "        \n",
    "        #compute current loss. Loss is a 0-dim tensor, so use tensor.item() to get the scalar value\n",
    "        epoch_loss += loss.item()  \n",
    "        \n",
    "        #backward propagation\n",
    "        loss.backward()\n",
    "        \n",
    "        #this represents one update on the weight/bias for a mini-batch(16 images in our case): \n",
    "        #weights[k] + alpha * d_weights[k]\n",
    "        optimizer.step()\n",
    "        \n",
    "        #show the update information\n",
    "        idx = i\n",
    "        length = len(dataloader)\n",
    "        \n",
    "        #display progress\n",
    "        if verbose:\n",
    "            update_info(idx, length, epoch_loss, acc/train_size, '  training')\n",
    "            \n",
    "    acc = acc/train_size\n",
    "    print('') \n",
    "    return epoch_loss/len(dataloader), acc\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "            if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "                print('[%d, %5d] loss: %.3f' %\n",
    "                      (epoch + 1, i + 1, train_loss / 2000))\n",
    "                running_loss = 0.0\n",
    "\n",
    "    print('Finished Training')\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def model_training(num_epochs, model, loss_fn, train_loader, optimizer, val_loader=None, verbose=True):\n",
    "    \n",
    "    train_batch_num = len(train_loader)\n",
    "    history = {}\n",
    "    history['train_loss'] = []\n",
    "    history['val_loss'] = []\n",
    "    history['train_acc'] = []\n",
    "    history['val_acc'] = []\n",
    "    \n",
    "    if val_loader is not None:\n",
    "        \n",
    "        val_batch_num = len(val_loader)\n",
    "        \n",
    "        print('Total Sample: Train on {} samples, validate on {} samples.'.\n",
    "             format(trainset.img_shape[0], valset.img_shape[0]))\n",
    "        \n",
    "        print(' Total Batch: Train on {} batches, validate on {} batches. {} samples/minibatch \\n'.\n",
    "         format(train_batch_num, val_batch_num, batch_size))\n",
    "    \n",
    "    else:\n",
    "        print('Total Sample: Train on {} samples.'.\n",
    "             format(train_batch_num*batch_size))\n",
    "        \n",
    "        print(' Total Batch: Train on {} batches, {} samples/minibatch \\n'.\n",
    "         format(train_batch_num, batch_size))\n",
    "    \n",
    "    \n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch+1, num_epochs))\n",
    "        train_loss, train_acc = train_per_epoch(model, loss_fn, train_loader, optimizer, verbose=verbose)\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        \n",
    "        \n",
    "        if val_loader is not None:\n",
    "            val_loss, val_acc = val_per_epoch(model, loss_fn, val_loader, verbose=verbose)\n",
    "            print('\\n        Training Loss: {:.4f}, Validation Loss: {:.4f}'.format(train_loss,val_loss))\n",
    "            print('         Training acc: {:.4f},  Validation acc: {:.4f}\\n'.format(train_acc,val_acc))\n",
    "            history['val_loss'].append(val_loss)\n",
    "            history['val_acc'].append(val_acc)\n",
    "                        \n",
    "        else:\n",
    "            print('\\n        Training Loss: {:.4f}\\n'.format(train_loss))\n",
    "            print('\\n         Training acc: {:.4f}\\n'.format(train_acc))\n",
    "        \n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training/Validating the model\n",
    "classes = ('airplane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "#若需要設置學習率衰減, 在外部定義管理函數 再灌到optimzer內輸入\n",
    "def lr_decay(parm):\n",
    "    pass\n",
    "\n",
    "if __name__ == '__main__' and run_model1 == True:\n",
    "\n",
    "    num_epochs = 500\n",
    "    learning_rate = 0.001\n",
    "\n",
    "    net = Net()\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        net = net.cuda()\n",
    "    print(net)\n",
    "    print('=================================================================')\n",
    "\n",
    "    #若使用 CrossEntropyLoss為Loss function, 則不應在網路的最後一層添加softmax\n",
    "    criterion = nn.CrossEntropyLoss() #loss function\n",
    "    optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "\n",
    "    #training and validating\n",
    "    hist1 = model_training(num_epochs, net, criterion, train_loader, optimizer, val_loader, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_training(1, net, criterion, train_loader, optimizer, val_loader, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "model = Net()\n",
    "model = torch.load(\"cifar10_ann.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 625/625 [00:26<00:00, 23.48it/s]\n"
     ]
    }
   ],
   "source": [
    "from spikingjelly.clock_driven import neuron, ann2snn\n",
    "import spikingjelly\n",
    "model_converter = ann2snn.Converter(mode='max', dataloader=train_loader)\n",
    "snn_model = model_converter(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(snn_model, \"cifar10_snn.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "val(snn_model, device, val_loader, T - 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    img = img    # unnormalize\n",
    "    #print(img)\n",
    "    npimg = img.numpy()\n",
    "    print(np.transpose(npimg, (1, 2, 0)).shape)\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == '__main__' and run_model1 == True:\n",
    "    dataiter = iter(test_loader)\n",
    "    images, labels = dataiter.next()\n",
    "\n",
    "    # print images\n",
    "    #imshow(torchvision.utils.make_grid(images))\n",
    "    #print(labels[0])\n",
    "    #print('GroundTruth: ', ' '.join('%5s' % classes[labels[j]] for j in range(5)))\n",
    "\n",
    "    for i in range(len(images)):\n",
    "        plot_image(i, images.permute(0, 2, 3, 1).numpy(), labels.numpy())\n",
    "\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        images = images.cuda()\n",
    "    outputs = net(images)\n",
    "\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "    print('Predicted: ', ' '.join('%5s' % classes[predicted[j]]\n",
    "                                  for j in range(5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model with dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the CNN\n",
    "class Net_dropout(nn.Module):\n",
    "    def __init__(self, dropout=0.2):\n",
    "        super(Net_dropout, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=5, stride=1, padding=2)\n",
    "        #3*32*32 -> 32*32*32\n",
    "        self.dropout1 = nn.Dropout(p=dropout)        \n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=(2,2), stride=2)\n",
    "        #32*32*32 -> 16*16*32\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, stride=1, padding=1)\n",
    "        #16*16*32 -> 16*16*64\n",
    "        self.dropout2 = nn.Dropout(p=dropout)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=(2,2), stride=2)\n",
    "        #16*16*64 -> 8*8*64\n",
    "        self.fc1 = nn.Linear(8*8*64, 1024)\n",
    "        self.dropout3 = nn.Dropout(p=dropout)\n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        self.dropout4 = nn.Dropout(p=dropout)\n",
    "        self.fc3 = nn.Linear(512, 10)\n",
    "            \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dropout1(self.conv1(x))\n",
    "        x = self.pool1(F.relu(x))\n",
    "        x = self.dropout2(self.conv2(x))\n",
    "        x = self.pool2(F.relu(x))\n",
    "        x = x.view(-1, self.num_flat_features(x)) \n",
    "        #self.num_flat_features(x) = 8*8*64 here.\n",
    "        #-1 means: get the rest a row (in this case is 16 mini-batches)\n",
    "        #pytorch nn only takes mini-batch as the input\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout3(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout4(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:] # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "\n",
    "\n",
    "        \n",
    "#display net architecture\n",
    "print(Net_dropout())  \n",
    "\n",
    "###\n",
    "# Keras structure\n",
    "#def build_basic_net(model, dropout=0.2):\n",
    "#    model.add(Conv2D(filters=32, kernel_size=(5,5), padding='same', strides=1,\n",
    "#                     input_shape=X_train.shape[1:], activation='relu'))\n",
    "#    model.add(Dropout(dropout))\n",
    "#    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "#    model.add(Conv2D(filters=64, kernel_size=(3,3), padding='same', strides=1,\n",
    "#                     input_shape=X_train.shape[1:], activation='relu'))\n",
    "#    model.add(Dropout(dropout))\n",
    "#    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "#    \n",
    "#    model.add(Flatten())\n",
    "#    model.add(Dense(1024, activation='relu'))\n",
    "#    model.add(Dropout(dropout))\n",
    "#    model.add(Dense(512, activation='relu'))\n",
    "#    model.add(Dropout(dropout))\n",
    "#    model.add(Dense(10,activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__' and run_model2 == True:\n",
    "\n",
    "    num_epochs = 10\n",
    "    learning_rate = 0.001\n",
    "\n",
    "    net = Net_dropout(0.2)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        net = net.cuda()\n",
    "    print(net)\n",
    "    print('=================================================================')\n",
    "\n",
    "    #若使用 CrossEntropyLoss為Loss function, 則不應在網路的最後一層添加softmax\n",
    "    criterion = nn.CrossEntropyLoss() #loss function\n",
    "    optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "\n",
    "    #training and validating\n",
    "    hist2 = model_training(num_epochs, net, criterion, train_loader, optimizer, val_loader, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_testing(model, loss_fn, dataloader, verbose=True):\n",
    "    Y_pred = []\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    epoch_loss = 0.0\n",
    "    acc = 0.0\n",
    "    test_size = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (feature, target) in enumerate(dataloader):\n",
    "            if torch.cuda.is_available():\n",
    "                feature = feature.cuda()\n",
    "                target = target.cuda()\n",
    "\n",
    "            outputs = model(feature)  #outputs.data.shape= batches_num * num_class\n",
    "            \n",
    "            #compute acc\n",
    "            _, pred = torch.max(outputs.data, 1)\n",
    "            correct = (pred == target).sum().item() #convert to number\n",
    "            test_size += target.size(0)\n",
    "            #print(test_size)\n",
    "            acc += correct\n",
    "            \n",
    "            loss = loss_fn(outputs, target)\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "            idx = i\n",
    "            length = len(dataloader)\n",
    "\n",
    "\n",
    "            #if torch.cuda.is_available():\n",
    "            #    pred = pred.cuda()\n",
    "            \n",
    "            #Pred labels \n",
    "            Y_pred += pred.cpu().numpy().tolist()\n",
    "            \n",
    "            if verbose:\n",
    "                update_info(idx, length, epoch_loss, acc/test_size, 'testing')    \n",
    "            \n",
    "    acc = acc/test_size\n",
    "    print('\\n\\n Accuracy of the network on the {} test images: {}%'.format(test_size, 100*acc))\n",
    "    \n",
    "    return Y_pred\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__' and run_model1 == True:\n",
    "    Y_pred1 = model_testing(net, criterion, test_loader, True)\n",
    "    \n",
    "if __name__ == '__main__' and run_model2 == True:\n",
    "    Y_pred2 = model_testing(net, criterion, test_loader, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss and accuracy curves for training and validation \n",
    "def loss_acc_plt(history):\n",
    "    fig, ax = plt.subplots(2,1)\n",
    "    ax[0].plot(history['train_loss'], color='b', label=\"Training loss\")\n",
    "    ax[0].plot(history['val_loss'], color='r', label=\"validation loss\",axes =ax[0])\n",
    "    legend = ax[0].legend(loc='best', shadow=True)\n",
    "\n",
    "    ax[1].plot(history['train_acc'], color='b', label=\"Training accuracy\")\n",
    "    ax[1].plot(history['val_acc'], color='r',label=\"Validation accuracy\")\n",
    "    legend = ax[1].legend(loc='best', shadow=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__' and run_model1 == True:\n",
    "    loss_acc_plt(hist1)\n",
    "    \n",
    "if __name__ == '__main__' and run_model2 == True:\n",
    "    loss_acc_plt(hist2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__' and run_model1 == True:\n",
    "    for i in range(10):\n",
    "        plot_image(i, test_loader.dataset.data, test_loader.dataset.label, Y_pred1)\n",
    "    \n",
    "if __name__ == '__main__' and run_model2 == True:\n",
    "    for i in range(10):\n",
    "        plot_image(i, test_loader.dataset.data, test_loader.dataset.label, Y_pred2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "if __name__ == '__main__' and run_model1 == True:\n",
    "    cm = confusion_matrix(Y_test, Y_pred1)\n",
    "    \n",
    "    \n",
    "if __name__ == '__main__' and run_model2 == True:\n",
    "    cm = confusion_matrix(Y_test, Y_pred2)\n",
    "\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,8))\n",
    "plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "\n",
    "plt.title('Confusion Matrix', fontsize=14)\n",
    "plt.colorbar()\n",
    "n_classes = cm.shape[0]\n",
    "range_class = range(n_classes)\n",
    "tick_marks = np.arange(len(range_class))\n",
    "plt.xticks(tick_marks, range_class, rotation=-45, fontsize=14)\n",
    "plt.yticks(tick_marks, range_class, fontsize=14)\n",
    "plt.xlabel('Predicted label', fontsize=14)\n",
    "plt.ylabel('True label', fontsize=14)\n",
    "\n",
    "for i in range_class:\n",
    "    for j in range_class:        \n",
    "        plt.text(j, i, cm[i,j], horizontalalignment=\"center\", fontsize=14, \n",
    "                color=\"white\" if i==j else \"black\")\n",
    "plt.plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(classes)):\n",
    "    correct = ((Y_test == i)*1) * ((np.array(Y_pred2) == Y_test)*1)\n",
    "    print('{}, {}: '.rjust(10).format(i, classes[i]) + '{}%'.\n",
    "          format(100*correct.sum()/Y_test[Y_test == i].shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kaggle Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile, os\n",
    "kaggle_test_file_path = \"../input/cifar10-object-recognition-in-images-zip-file/train_test/test.zip\"\n",
    "zipFile = zipfile.ZipFile(kaggle_test_file_path)\n",
    "\n",
    "zipFile_img = zipFile.open('test/1.png')\n",
    "Image.open(zipFile_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The order of the filename in ZIP info needs to be sorted first, that's why I use re here.\n",
    "import re\n",
    "\n",
    "# make train & test dataset\n",
    "class Cifar10_kaggle_test(Dataset):\n",
    "    def __init__(self, file_url):\n",
    "        self.file_url = file_url\n",
    "        self.img_size = 0\n",
    "        self.len = 0\n",
    "        \n",
    "        with zipfile.ZipFile(self.file_url) as archive:\n",
    "            self.infolist = archive.infolist()\n",
    "            \n",
    "            for i, entry in enumerate(self.infolist):\n",
    "                img_idx = int(re.split(r'[/.]', entry.filename)[1])\n",
    "                with archive.open(entry) as file:\n",
    "                    img = Image.open(file)\n",
    "                    \n",
    "                    \n",
    "                    #def images array\n",
    "                    if i == 0:\n",
    "                        self.img_size = img.size\n",
    "                        self.len = len(self.infolist)\n",
    "                        images = np.zeros((self.len, self.img_size[0], self.img_size[1], 3))\n",
    "                        img_indices = np.zeros((self.len))\n",
    "                    img_array = np.asarray(img)\n",
    "                    images[i] = img_array\n",
    "                    img_indices[i] = img_idx\n",
    "                    \n",
    "                    #print(\"\\r loading ... {} / {}\".format(i, self.len), end=\"\")\n",
    "        #print('\\n Done!')            \n",
    "        self.images = images.astype('uint8')\n",
    "        self.img_indices = img_indices.astype('int64')\n",
    "        del img, img_array\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = Image.fromarray(self.images[index])\n",
    "        img_to_tensor = transforms.ToTensor()\n",
    "        img = img_to_tensor(img)\n",
    "        return img\n",
    "    \n",
    "kaggle_test_url = \"../input/cifar10-object-recognition-in-images-zip-file/train_test/test.zip\"\n",
    "kaggle_test = Cifar10_kaggle_test(kaggle_test_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(kaggle_test[0].shape)\n",
    "\n",
    "batch_size=64\n",
    "num_workers=1\n",
    "kaggle_test_loader = DataLoader(dataset=kaggle_test,\n",
    "                                batch_size=batch_size, \n",
    "                                shuffle=False,\n",
    "                                num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kaggle_model_testing(model, dataloader, verbose=True):\n",
    "    Y_pred = []\n",
    "    #correct = 0\n",
    "    #total = 0\n",
    "    #epoch_loss = 0.0\n",
    "    #acc = 0.0\n",
    "    test_size = 0\n",
    "    with torch.no_grad():\n",
    "        for i, feature in enumerate(dataloader):\n",
    "            if torch.cuda.is_available():\n",
    "                feature = feature.cuda()\n",
    "                #target = target.cuda()\n",
    "\n",
    "            outputs = model(feature)  #outputs.data.shape= batches_num * num_class\n",
    "            \n",
    "            #compute acc\n",
    "            _, pred = torch.max(outputs.data, 1)\n",
    "            #correct = (pred == target).sum().item() #convert to number\n",
    "            test_size += feature.size(0)\n",
    "            #print(test_size)\n",
    "            #acc += correct\n",
    "            \n",
    "            #loss = loss_fn(outputs, target)\n",
    "            #epoch_loss += loss.item()\n",
    "            \n",
    "            idx = i\n",
    "            length = len(dataloader)\n",
    "\n",
    "\n",
    "            #if torch.cuda.is_available():\n",
    "            #    pred = pred.cuda()\n",
    "            \n",
    "            #Pred labels \n",
    "            Y_pred += pred.cpu().numpy().tolist()\n",
    "            \n",
    "            if verbose:\n",
    "                #update_info(idx, length, epoch_loss, acc/test_size, 'testing')\n",
    "                print('\\r Now predicting on the {}/{} batch...'.format(idx,length), end='')\n",
    "            \n",
    "    #acc = acc/test_size\n",
    "    print('\\n Done!')\n",
    "    \n",
    "    return Y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_kaggle = kaggle_model_testing(net, kaggle_test_loader)\n",
    "\n",
    "print(len(Y_pred_kaggle))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_test_df = (pd.DataFrame({'label':Y_pred_kaggle}).applymap(lambda x:classes[x]))\n",
    "kaggle_test_df.insert(0, \"id\", kaggle_test.img_indices.astype('int64'))\n",
    "kaggle_test_df = kaggle_test_df.sort_values(by='id', ascending=True)\n",
    "\n",
    "#pd.DataFrame({\"ImageId\": list(range(1,len(preds)+1)), \"Label\": preds})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_test_df.to_csv(\"cifar10_kaggle_pytorch.csv\", index=False, header=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:SpikingJellyenv] *",
   "language": "python",
   "name": "conda-env-SpikingJellyenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
